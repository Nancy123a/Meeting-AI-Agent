{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae1434c-897d-40bc-8b58-56c95e065ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent,Task,Crew,Process,LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2097029b-7643-4ee1-9b84-09e025eb0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agentops \n",
    "import os\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ba43861-b522-4a3f-aa1a-3983f7106773",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AGENTOPS_API_KEY\"]= \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef6e222-1f8b-42a1-b65b-1763074688c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=af37060f-236c-4cb1-8300-af4cacb6f2fa\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agentops.session.Session at 0x151f00830>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentops.init(\n",
    "    api_key=\"\",\n",
    "    skip_auto_end_session=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb6c557-22dd-436e-bb99-b98e18166978",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"./ai-agent-output\"\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "basic_llm=LLM(model=\"gpt-4\",temperature=0)\n",
    "search_client=TavilyClient(api_key=\"\")\n",
    "scrape_client=Client(api_key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4dd173-0338-4691-bd42-9dd557014715",
   "metadata": {},
   "source": [
    "## Designing first agent:\n",
    "To provide a list of suggested searched queries to be passed to the search engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8412d137-1f18-4a97-8527-9d0ff5befb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff5d0e1-8bbf-4ade-ab54-971f91d11e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keywords=10\n",
    "\n",
    "class SuggestedSearchQueries(BaseModel):\n",
    "    queries:List[str]=Field(...,title=\"Suggested search queries to be passed the search engine.\",min_items=1,max_items=no_keywords)\n",
    "    \n",
    "\n",
    "search_queries_recommendation_agent=Agent(\n",
    "    role=\"Search Queries Recommendation Agent\",\n",
    "    goal=\"\\n\".join([ \n",
    "                \"To provide a list of suggested search queries to be passed to the search engine.\",\n",
    "                \"The queries must be varied and looking for specific items.\"\n",
    "            ]),   \n",
    "    backstory=\"The agent is desgined to help in looking for products by providing a list of suggested search queries to be passed to the search engine based on the context provided.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "search_queries_recommendation_task=Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"Ranyx is looking to buy {product_name} at the best prices (value for price strategy).\",\n",
    "        \"The company target any of these websites to buy from:{websites_list}\",\n",
    "        \"The company wants to reach all available products on the internet to be compared later in another stage.\",\n",
    "        \"The stores must sell the product in {country_name}\",\n",
    "        \"Generate at maximum {no_keywords} queries\",\n",
    "        \"The search keywords must be in {language} language\"\n",
    "        \"The search query must reach an ecommerce webpage for product, and not just a blog page.\"\n",
    "    ]),\n",
    "    expected_output=\"A JSON object containing a list of suggested search queries\",\n",
    "    output_json=SuggestedSearchQueries,\n",
    "    output_file=os.path.join(output_dir,\"step_1_suggested_search_queries.json\"),\n",
    "    agent=search_queries_recommendation_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec21cb7-0032-4ab6-9de7-e0ed3e0fab8a",
   "metadata": {},
   "source": [
    "## Run the AI Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc07e2f6-dbed-4432-88b0-63e2b725ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "ranyx_crew=Crew(\n",
    "    agents=[\n",
    "        search_queries_recommendation_agent,\n",
    "    ],\n",
    "    tasks=[\n",
    "        search_queries_recommendation_task\n",
    "    ],\n",
    "    process=Process.sequential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa198d64-ab70-4063-b613-f225c698f489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSearch Queries Recommendation Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mRanyx is looking to buy coffee machine for the office at the best prices (value for price strategy).\n",
      "The company target any of these websites to buy from:['www.amazon.eg', 'www.jumia.com.eg', 'www.noon.com/egypt-en']\n",
      "The company wants to reach all available products on the internet to be compared later in another stage.\n",
      "The stores must sell the product in Egypt\n",
      "Generate at maximum 10 queries\n",
      "The search query must reach an ecommerce webpage for product, and not just a blog page.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSearch Queries Recommendation Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mRanyx is looking to buy coffee machine for the office at the best prices (value for price strategy).\n",
      "The company target any of these websites to buy from:['www.amazon.eg', 'www.jumia.com.eg', 'www.noon.com/egypt-en']\n",
      "The company wants to reach all available products on the internet to be compared later in another stage.\n",
      "The stores must sell the product in Egypt\n",
      "Generate at maximum 10 queries\n",
      "The search query must reach an ecommerce webpage for product, and not just a blog page.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSearch Queries Recommendation Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mRanyx is looking to buy coffee machine for the office at the best prices (value for price strategy).\n",
      "The company target any of these websites to buy from:['www.amazon.eg', 'www.jumia.com.eg', 'www.noon.com/egypt-en']\n",
      "The company wants to reach all available products on the internet to be compared later in another stage.\n",
      "The stores must sell the product in Egypt\n",
      "Generate at maximum 10 queries\n",
      "The search query must reach an ecommerce webpage for product, and not just a blog page.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/llms/openai/openai.py:691\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/llms/openai/openai.py:619\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    607\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    609\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m     },\n\u001b[1;32m    616\u001b[0m )\n\u001b[1;32m    618\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_sync_openai_chat_completion_request(\n\u001b[1;32m    620\u001b[0m         openai_client\u001b[38;5;241m=\u001b[39mopenai_client,\n\u001b[1;32m    621\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    622\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[1;32m    624\u001b[0m )\n\u001b[1;32m    626\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mmodel_call_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/llms/openai/openai.py:439\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/llms/openai/openai.py:421\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    858\u001b[0m validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    861\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    862\u001b[0m         {\n\u001b[1;32m    863\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    864\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    866\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    867\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    868\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    869\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    872\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    873\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    874\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    877\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m    878\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    879\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m    880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    882\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    883\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    884\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    885\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    886\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    888\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    889\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    890\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    892\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    893\u001b[0m         },\n\u001b[1;32m    894\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    895\u001b[0m     ),\n\u001b[1;32m    896\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    897\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    898\u001b[0m     ),\n\u001b[1;32m    899\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    900\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    902\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m )\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    966\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/main.py:1612\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1608\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1609\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   1610\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[1;32m   1611\u001b[0m     )\n\u001b[0;32m-> 1612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/main.py:1585\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1585\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai_chat_completions\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[1;32m   1586\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1587\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1588\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1589\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m   1590\u001b[0m         print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[1;32m   1591\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1592\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   1593\u001b[0m         acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[1;32m   1594\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[1;32m   1595\u001b[0m         optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[1;32m   1596\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[1;32m   1597\u001b[0m         logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[1;32m   1598\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m         custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict,\n\u001b[1;32m   1600\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,  \u001b[38;5;66;03m# pass AsyncOpenAI, OpenAI client\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m   1602\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/llms/openai/openai.py:701\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[1;32m    700\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    702\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39merror_text, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[1;32m    703\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crew_results\u001b[38;5;241m=\u001b[39mranyx_crew\u001b[38;5;241m.\u001b[39mkickoff(\n\u001b[1;32m      2\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoffee machine for the office\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebsites_list\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwww.amazon.eg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwww.jumia.com.eg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwww.noon.com/egypt-en\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEgypt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/crew.py:548\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    545\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 548\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sequential_process()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    550\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/crew.py:655\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/crew.py:756\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    753\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    755\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 756\u001b[0m task_output \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mexecute_sync(\n\u001b[1;32m    757\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent_to_use,\n\u001b[1;32m    758\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    759\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools_for_task,\n\u001b[1;32m    760\u001b[0m )\n\u001b[1;32m    761\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_core(agent, context, tools)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    367\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    368\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    369\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    382\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agent.py:344\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agent.py:333\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    330\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    334\u001b[0m         {\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: task_prompt,\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names,\n\u001b[1;32m    337\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description,\n\u001b[1;32m    338\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: task\u001b[38;5;241m.\u001b[39mhuman_input,\n\u001b[1;32m    339\u001b[0m         }\n\u001b[1;32m    340\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_start_logs()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 102\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n\u001b[1;32m    105\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_human_feedback(formatted_answer)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:206\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop(formatted_answer)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_logs(formatted_answer)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_answer\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentFinish):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 115\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    117\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    122\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m                 color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m             )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/crewai/llm.py:181\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Remove None values to avoid passing unnecessary parameters\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 181\u001b[0m     response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/agentops/llms/providers/litellm.py:195\u001b[0m, in \u001b[0;36mLiteLLMProvider._override_completion.<locals>.patched_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_response(result_model, kwargs, init_timestamp, session\u001b[38;5;241m=\u001b[39msession)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# prompt_override = fetch_prompt_override_from_time_travel_cache(kwargs)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# if prompt_override:\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#     kwargs[\"messages\"] = prompt_override[\"messages\"]\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Call the original function with its original arguments\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_create(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_response(result, kwargs, init_timestamp, session\u001b[38;5;241m=\u001b[39msession)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/utils.py:1030\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1027\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1028\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1029\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/utils.py:906\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 906\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    907\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/main.py:2967\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 2967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   2968\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2969\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2970\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2971\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2972\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2973\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2189\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:292\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid_request_error\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_not_found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[1;32m    290\u001b[0m ):\n\u001b[1;32m    291\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[1;32m    293\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    294\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    295\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    296\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    297\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA timeout occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n\u001b[1;32m    300\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "crew_results=ranyx_crew.kickoff(\n",
    "    inputs={\n",
    "        \"product_name\":\"coffee machine for the office\",\n",
    "        \"websites_list\":[\"www.amazon.eg\",\"www.jumia.com.eg\",\"www.noon.com/egypt-en\"],\n",
    "        \"country_name\":\"Egypt\",\n",
    "        \"no_keywords\":10,\n",
    "        \"language\":\"Arabic\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c809c3b2-8d9a-48b6-85fb-a2d18a26f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client.search(\"coffee machine site:amazon.eg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3af0d0-2e58-4bb2-8a06-eccaf1027024",
   "metadata": {},
   "source": [
    "## Agent B\n",
    "to search google for products based on the suggested search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e9d10-b793-41bc-abe9-8b1630e5e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleSearchResult(BaseModel):\n",
    "    title:str,\n",
    "    url:str = Field(...,title=\"the page url\")\n",
    "    content:str,\n",
    "    score:float,\n",
    "    search_query:str\n",
    "    \n",
    "class AllSearchResults(BaseModel):\n",
    "    results:List[SingleSearchResult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9347378-ed18-4f75-918b-8841815c0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_engine_tool(query:str):\n",
    "    \"\"\"Useful to find current information about any query related pages using a search engine\"\"\"\n",
    "    return search_client.search(query)\n",
    "\n",
    "\n",
    "search_engine_agent=Agent(\n",
    "    role=\"Search Engine Agent\",\n",
    "    goal=\"To search for products based on the suggested search query\",\n",
    "    backstory=\"The agent is desgined to help in looking for products by searching for products based on the suggested search queries.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    "    tools=[search_engine_tool]\n",
    ")\n",
    "search_engine_task=Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"The Task is to search for products based on the suggested search queries.\",\n",
    "        \"You have to collect results from multiple search queries\",\n",
    "        \"ignore any susbicious links or not an ecommerce single product website link\",\n",
    "        \"ignore any search results with confidence score less than ({score_th})\",\n",
    "        \"The search product will be used to compare prices of product from different websites.\"\n",
    "    ]),\n",
    "    expected_outputs=\"A JSON object containing the search results.\",\n",
    "    output_json=AllSearchResults,\n",
    "    output_file=os.path.join(output_dir,\"step_1_suggested_search_queries.json\"),\n",
    "    agent=search_engine_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34018e5-31ef-44e7-8762-538d96b3441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranyx_crew=Crew(\n",
    "    agents=[\n",
    "        search_queries_recommendation_agent,\n",
    "        search_engine_agent\n",
    "    ],\n",
    "    tasks=[\n",
    "        search_queries_recommendation_task,\n",
    "        search_engine_task\n",
    "    ],\n",
    "    process=Process.sequential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27909378-c206-453d-bc7f-5fef40168b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_results=ranyx_crew.kickoff(\n",
    "    inputs={\n",
    "        \"product_name\":\"coffee machine for the office\",\n",
    "        \"websites_list\":[\"www.amazon.eg\",\"www.jumia.com.eg\",\"www.noon.com/egypt-en\"],\n",
    "        \"country_name\":\"Egypt\",\n",
    "        \"no_keywords\":10,\n",
    "        \"language\":\"Arabic\",\n",
    "        \"score_th\":0.10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e811a-631a-414e-86a9-60944019eb4e",
   "metadata": {},
   "source": [
    "## Agent C:\n",
    "To extract product details from any web page + HTML Parser code/tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c6762-c92e-4880-81e2-fd010bcd2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapegraph_py import Client\n",
    "import json\n",
    "\n",
    "def web_scraping_tool(page_url:str,required_fields:list):\n",
    "    \"\"\" AN ai TOOL TAHT HELP A PERSON TO SCRAPE A WEB PAGE \"\"\"   \n",
    "    details=scrape_client.smartscraper(\n",
    "        website_url=page_url,\n",
    "        user_prompt=\"Extract \"+json.dumps(required_fields,ensure_ascii=False)+\" From the web page\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"page_url\":page_url,\n",
    "        \"details\":details\n",
    "    }\n",
    "\n",
    "scraping_agent=Agent(\n",
    "    role=\"Web scraping agent\",\n",
    "    goal=\"To extract details from any website\",\n",
    "    backstory=\"The agent is desgined to help in looking for required values from any website url.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    "    tools=[search_engine_tool]\n",
    ")\n",
    "\n",
    "search_engine_task=Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"The Task is to search for products based on the suggested search queries.\",\n",
    "        \"You have to collect results from multiple search queries\",\n",
    "        \"ignore any susbicious links or not an ecommerce single product website link\",\n",
    "        \"ignore any search results with confidence score less than ({score_th})\",\n",
    "        \"The search product will be used to compare prices of product from different websites.\"\n",
    "    ]),\n",
    "    expected_outputs=\"A JSON object containing the search results.\",\n",
    "    output_json=AllSearchResults,\n",
    "    output_file=os.path.join(output_dir,\"step_1_suggested_search_queries.json\"),\n",
    "    agent=search_engine_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89667e79-4ba5-43e3-9a86-f9917be02310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_details=web_scraping_tool(\n",
    " #   page_url=\"\",\n",
    "  #  required_fields=[\"product_title\",\"current_price\",\"old_price\"]\n",
    "#)\n",
    "\n",
    "class ProductSpecs(BasedModel):\n",
    "    specification_name:str\n",
    "    specification_value:str\n",
    "\n",
    "class SingleExtractedProduct(BaseModel):\n",
    "    product_url:str=Field(...,title=\"The original url of the product page\")\n",
    "    product_title:str=Field(...,title=\"The title of the product\")\n",
    "    product_image_url:str=Field(...,title=\"The url of the product image\")\n",
    "    product_url:str=Field(...,\"The url of the product\")\n",
    "    product_current_price:float=Field(...,\"The current price of the product\")\n",
    "    product_original_price:float=Field(...,\"The original price of the product before discount. Set to None if no discount.\",default=None)\n",
    "    product_discount_percentage:float=Field(...,\"The discount percentage of the product. Set to None if no discount.\",default=None)\n",
    "\n",
    "    product_specs=List[ProductSpec]=Field(...,title=\"The specification of the product. Focus on the most important specs to compare.\",min_items=1,max_items=5)\n",
    "    agent_recommendation_rank:int=Field(...,title=\"The rank of the product (out of 5, Higher is Better) in the recommendation list ordering from the best to the worst.\")\n",
    "    agent_recommendation_notes:List[str]=Field(...,title=\"Why would you recommnend or not reciommend this product.\")\n",
    "\n",
    "class AllExtractedProducts(BaseModel):\n",
    "    products: List[SingleExtractedProduuct]\n",
    "\n",
    "@tool\n",
    "def web_scraping_tool(page_url:str):\n",
    "    details=scrape_client.smartscraper(\n",
    "        website_url=website_url,\n",
    "        user_promot=\"Extract '''json\\n\"+AllExtractedProducts.schema_json()+\"'''\\n the web page\"\n",
    "    )\n",
    "    return{\n",
    "        \"page_url\":page_url,\n",
    "        \"details\":details\n",
    "    }\n",
    "\n",
    "scraping_agent=Agent(\n",
    "    role=\"Web scraping agent\",\n",
    "    goal=\"To extract details from any website\",\n",
    "    backstory=\"The agent is desgined to help in looking for required values from any website url.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True,\n",
    "    tools=[web_scraping_tool]\n",
    ")\n",
    "\n",
    "scraping_task=Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"The Task is to extract the product from any ecommerce store page url.\",\n",
    "        \"The task has to collect results from multiple pages urls.\",\n",
    "        \"Collect the {top_recommendation_no} products from the search results.\"\n",
    "    ]),\n",
    "    expected_outputs=\"A JSON object containing the products details.\",\n",
    "    output_json=AllExtractedProducts,\n",
    "    output_file=os.path.join(output_dir,\"step_3_search_results.json\"),\n",
    "    agent=sscraping_agent\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74daa3dd-a00a-4f5e-b715-6aecd94a6234",
   "metadata": {},
   "source": [
    "## AGENT D\n",
    "To generate a professional procurement report + HTML generator code/tool \n",
    "It generates a visual reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e4ceb-2cf6-4e1f-a04d-60d4ed82364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource\n",
    "\n",
    "procurement_report_author_agent=Agent(\n",
    "    role=\"Procurement Report Author Agent\",\n",
    "    goal=\"To generate a professional HTMNL page for the procurement report\",\n",
    "    backstory=\"The agent is designed to assist in generating a professional HTML page for the procurement report after looking into a list of products.\",\n",
    "    llm=basic_llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "procurement_report_author_task=Task(\n",
    "    description=\"\\n\".join([\n",
    "        \"The task is to generate a professional HTML page for the procurement report.\",\n",
    "        \"You have to use Bootstrap CSS framework for a better UI.\",\n",
    "        \"Use the provided context about the comapny to amke a specializied report.\"\n",
    "        \"The report will include the search results and prices of products from different websites.\",\n",
    "        \"The report should be structured with the following sections:\",\n",
    "        \"1. Executive Summary: A brief overview of the procurement process and key findings.\",\n",
    "        \"2. Introduction: An introduction to the purpose and scope of the report.\",\n",
    "        \"3. Methodology: A description of the methods used to gather and compare prices.\",\n",
    "        \"4. Findings: Detailed comparison of prices from different websites, including tables and charts.\",\n",
    "        \"5. Analysis: An analysis of the findings, highlighting any significant trends or observations.\",\n",
    "        \"6. Recommendations: Suggestions for procurement based on the analysis.\",\n",
    "        \"7. Conclusion: A summary of the report and final thoughts.\",\n",
    "        \"8. Appendices: Any additional information, such as raw data or supplementary materials.\",\n",
    "    ]),\n",
    "    expected_output=\"A professional HTML page for the procurement report.\",\n",
    "    output_file=os.path.join(output_dir,\"step_4_procurement_report.html\"),\n",
    "    agent=procurement_report_author_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bd037-5ea7-4adf-adbb-3920b3fdf42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "about_company = \"Rankyx is a company that provides AI solutions to help websites refine their search and recommendation systems.\"\n",
    "\n",
    "company_context = StringKnowledgeSource(\n",
    "    content=about_company\n",
    ")\n",
    "\n",
    "## run the crewai\n",
    "rankyx_crew=Crew(\n",
    "    agents=[\n",
    "        search_queries_recommendation_agent,\n",
    "        search_engine_agent,\n",
    "        scraping_agent,\n",
    "        procurement_report_author_agent\n",
    "        \n",
    "    ],\n",
    "    tasks=[\n",
    "        search_queries_recommendation_task,\n",
    "        search_engine_task,\n",
    "        scraping_task,\n",
    "        procurement_report_author_task\n",
    "    ],\n",
    "    process=Process.sequential,\n",
    "    knowledge_sources=[comapny_context]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813373a-b31b-4064-b213-94a5d72cc604",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_results=rankyx_crew.kickoff(\n",
    "    inputs={\n",
    "        \"product_name\":\"coffee machine for the office\",\n",
    "        \"websites_list\":[\"www.amazon.eg\",\"www.jumia.com.eg\",\"www.noon.com/egypt-en\"],\n",
    "        \"country name\":\"Egypt\",\n",
    "        \"no_keywords\":10,\n",
    "        \"language\":\"English\",\n",
    "        \"score_th\":0.10,\n",
    "        \"top_recommendation_no\":10\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5baea97-0790-4ac9-9204-062431768599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
